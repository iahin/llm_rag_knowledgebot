[
    {
        "label": "OllamaLLM",
        "importPath": "langchain_ollama.llms",
        "description": "langchain_ollama.llms",
        "isExtraImport": true,
        "detail": "langchain_ollama.llms",
        "documentation": {}
    },
    {
        "label": "ChatPromptTemplate",
        "importPath": "langchain_core.prompts",
        "description": "langchain_core.prompts",
        "isExtraImport": true,
        "detail": "langchain_core.prompts",
        "documentation": {}
    },
    {
        "label": "retriever",
        "importPath": "vector_csv",
        "description": "vector_csv",
        "isExtraImport": true,
        "detail": "vector_csv",
        "documentation": {}
    },
    {
        "label": "SelfQueryRetriever",
        "importPath": "langchain.retrievers.self_query.base",
        "description": "langchain.retrievers.self_query.base",
        "isExtraImport": true,
        "detail": "langchain.retrievers.self_query.base",
        "documentation": {}
    },
    {
        "label": "AttributeInfo",
        "importPath": "langchain.chains.query_constructor.base",
        "description": "langchain.chains.query_constructor.base",
        "isExtraImport": true,
        "detail": "langchain.chains.query_constructor.base",
        "documentation": {}
    },
    {
        "label": "StructuredQueryOutputParser",
        "importPath": "langchain.chains.query_constructor.base",
        "description": "langchain.chains.query_constructor.base",
        "isExtraImport": true,
        "detail": "langchain.chains.query_constructor.base",
        "documentation": {}
    },
    {
        "label": "get_query_constructor_prompt",
        "importPath": "langchain.chains.query_constructor.base",
        "description": "langchain.chains.query_constructor.base",
        "isExtraImport": true,
        "detail": "langchain.chains.query_constructor.base",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "OllamaEmbeddings",
        "importPath": "langchain_ollama",
        "description": "langchain_ollama",
        "isExtraImport": true,
        "detail": "langchain_ollama",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_chroma",
        "description": "langchain_chroma",
        "isExtraImport": true,
        "detail": "langchain_chroma",
        "documentation": {}
    },
    {
        "label": "Chroma",
        "importPath": "langchain_chroma",
        "description": "langchain_chroma",
        "isExtraImport": true,
        "detail": "langchain_chroma",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "Document",
        "importPath": "langchain_core.documents",
        "description": "langchain_core.documents",
        "isExtraImport": true,
        "detail": "langchain_core.documents",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "PyPDFLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "Docx2txtLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "TextLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "CSVLoader",
        "importPath": "langchain_community.document_loaders",
        "description": "langchain_community.document_loaders",
        "isExtraImport": true,
        "detail": "langchain_community.document_loaders",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain_text_splitters",
        "description": "langchain_text_splitters",
        "isExtraImport": true,
        "detail": "langchain_text_splitters",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "model = OllamaLLM(model=\"llama3.2\", temperature=0)\nmetadata_field_info = [\n    AttributeInfo(\n        name=\"rating\",\n        description=\"The specific rating of reviews to be searched\",\n        type=\"string\",\n    ),\n    # AttributeInfo(\n    #     name=\"region\",\n    #     description=\"The name of the UK region to be searched\",",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "metadata_field_info",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "metadata_field_info = [\n    AttributeInfo(\n        name=\"rating\",\n        description=\"The specific rating of reviews to be searched\",\n        type=\"string\",\n    ),\n    # AttributeInfo(\n    #     name=\"region\",\n    #     description=\"The name of the UK region to be searched\",\n    #     type=\"string\",",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "template",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "template = \"\"\"\nYou are an expert in answering question about a pizza restaurent.\nHere are some relevant reviews: {reviews}\nhere is the question to answer: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nchain = prompt | model\nwhile True:\n    print(\"--------------------------------\")\n    question = input(\"Please enter your question about the pizza restaurant (or type 'exit' to quit): \")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "prompt",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "prompt = ChatPromptTemplate.from_template(template)\nchain = prompt | model\nwhile True:\n    print(\"--------------------------------\")\n    question = input(\"Please enter your question about the pizza restaurant (or type 'exit' to quit): \")\n    print(\"\\n\\n\")\n    if question.lower() == 'exit':\n        break\n    reviews = retriever.invoke(question)\n    self_query_retriever = SelfQueryRetriever.from_llm(",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "chain",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "chain = prompt | model\nwhile True:\n    print(\"--------------------------------\")\n    question = input(\"Please enter your question about the pizza restaurant (or type 'exit' to quit): \")\n    print(\"\\n\\n\")\n    if question.lower() == 'exit':\n        break\n    reviews = retriever.invoke(question)\n    self_query_retriever = SelfQueryRetriever.from_llm(\n        llm, uk_with_metadata_collection, question, metadata_field_info, verbose=True",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "vector_csv",
        "description": "vector_csv",
        "peekOfCode": "BASE_DIR = Path(__file__).resolve().parent\nCSV_PATH = BASE_DIR / \"knowledgebase\" / \"realistic_restaurant_reviews.csv\"\nDB_DIR = BASE_DIR / \"chroma_langchain_db\"\ndf = pd.read_csv(CSV_PATH)\nembeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\ndb_location = DB_DIR\nadd_documents = not os.path.exists(db_location)\nif add_documents:\n    documents = []\n    ids = []",
        "detail": "vector_csv",
        "documentation": {}
    },
    {
        "label": "CSV_PATH",
        "kind": 5,
        "importPath": "vector_csv",
        "description": "vector_csv",
        "peekOfCode": "CSV_PATH = BASE_DIR / \"knowledgebase\" / \"realistic_restaurant_reviews.csv\"\nDB_DIR = BASE_DIR / \"chroma_langchain_db\"\ndf = pd.read_csv(CSV_PATH)\nembeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\ndb_location = DB_DIR\nadd_documents = not os.path.exists(db_location)\nif add_documents:\n    documents = []\n    ids = []\n    for index, row in df.iterrows():",
        "detail": "vector_csv",
        "documentation": {}
    },
    {
        "label": "DB_DIR",
        "kind": 5,
        "importPath": "vector_csv",
        "description": "vector_csv",
        "peekOfCode": "DB_DIR = BASE_DIR / \"chroma_langchain_db\"\ndf = pd.read_csv(CSV_PATH)\nembeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\ndb_location = DB_DIR\nadd_documents = not os.path.exists(db_location)\nif add_documents:\n    documents = []\n    ids = []\n    for index, row in df.iterrows():\n        document = Document(",
        "detail": "vector_csv",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "vector_csv",
        "description": "vector_csv",
        "peekOfCode": "df = pd.read_csv(CSV_PATH)\nembeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\ndb_location = DB_DIR\nadd_documents = not os.path.exists(db_location)\nif add_documents:\n    documents = []\n    ids = []\n    for index, row in df.iterrows():\n        document = Document(\n            page_content=row[\"Title\"] + \"\\n\" +",
        "detail": "vector_csv",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "vector_csv",
        "description": "vector_csv",
        "peekOfCode": "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\ndb_location = DB_DIR\nadd_documents = not os.path.exists(db_location)\nif add_documents:\n    documents = []\n    ids = []\n    for index, row in df.iterrows():\n        document = Document(\n            page_content=row[\"Title\"] + \"\\n\" +\n            row[\"Review\"] + \" rating is \" + str(row[\"Rating\"]),",
        "detail": "vector_csv",
        "documentation": {}
    },
    {
        "label": "db_location",
        "kind": 5,
        "importPath": "vector_csv",
        "description": "vector_csv",
        "peekOfCode": "db_location = DB_DIR\nadd_documents = not os.path.exists(db_location)\nif add_documents:\n    documents = []\n    ids = []\n    for index, row in df.iterrows():\n        document = Document(\n            page_content=row[\"Title\"] + \"\\n\" +\n            row[\"Review\"] + \" rating is \" + str(row[\"Rating\"]),\n            metadata={\"rating\": row[\"Rating\"], \"date\": row[\"Date\"]},",
        "detail": "vector_csv",
        "documentation": {}
    },
    {
        "label": "add_documents",
        "kind": 5,
        "importPath": "vector_csv",
        "description": "vector_csv",
        "peekOfCode": "add_documents = not os.path.exists(db_location)\nif add_documents:\n    documents = []\n    ids = []\n    for index, row in df.iterrows():\n        document = Document(\n            page_content=row[\"Title\"] + \"\\n\" +\n            row[\"Review\"] + \" rating is \" + str(row[\"Rating\"]),\n            metadata={\"rating\": row[\"Rating\"], \"date\": row[\"Date\"]},\n            id=str(index)",
        "detail": "vector_csv",
        "documentation": {}
    },
    {
        "label": "vector_store",
        "kind": 5,
        "importPath": "vector_csv",
        "description": "vector_csv",
        "peekOfCode": "vector_store = Chroma(\n    collection_name=\"pizza_reviews\",\n    persist_directory=db_location,\n    embedding_function=embeddings\n)\nif add_documents:\n    vector_store.add_documents(documents=documents, ids=ids)\nretriever = vector_store.as_retriever(\n    search_type=\"similarity\",  # search algorithm\n    search_kwargs={\"k\": 10,  # number of relevent docs to return",
        "detail": "vector_csv",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "vector_csv",
        "description": "vector_csv",
        "peekOfCode": "retriever = vector_store.as_retriever(\n    search_type=\"similarity\",  # search algorithm\n    search_kwargs={\"k\": 10,  # number of relevent docs to return\n                   'filter': {'rating': 1} # Set filters\n                   },\n)\nresults = retriever.invoke(\"bad review dont like \")\nfor r in results:\n    print(r)",
        "detail": "vector_csv",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "vector_csv",
        "description": "vector_csv",
        "peekOfCode": "results = retriever.invoke(\"bad review dont like \")\nfor r in results:\n    print(r)",
        "detail": "vector_csv",
        "documentation": {}
    },
    {
        "label": "load_docs_from_dir",
        "kind": 2,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "def load_docs_from_dir(dir_path: Path) -> List[Document]:\n    \"\"\"Load PDFs, DOCX, TXT/MD, and CSVs from a directory.\"\"\"\n    all_docs: List[Document] = []\n    for path in dir_path.rglob(\"*\"):\n        if not path.is_file():\n            continue\n        suffix = path.suffix.lower()\n        docs: List[Document] = []\n        # --- PDF ---\n        if suffix == \".pdf\":",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "chunk",
        "kind": 2,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "def chunk(docs: List[Document]) -> List[Document]:\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200,\n        length_function=len,\n        add_start_index=True,\n        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \": \", \", \", \" \", \"\"],\n    )\n    return splitter.split_documents(docs)\n# --- Load and chunk ---",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "BASE_DIR = Path(__file__).resolve().parent\nDATA_DIR = BASE_DIR / \"knowledgebase\"\nDB_DIR = BASE_DIR / \"chroma_langchain_db_all\"\nembeddings = OllamaEmbeddings(model=\"llama3.2\")\npersist_dir = DB_DIR\n# add_documents = not os.path.exists(persist_dir) # If DB does not exist, create it.\nadd_documents = os.path.exists(persist_dir)\n# --- Texts Loaders ---\ndef load_docs_from_dir(dir_path: Path) -> List[Document]:\n    \"\"\"Load PDFs, DOCX, TXT/MD, and CSVs from a directory.\"\"\"",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "DATA_DIR",
        "kind": 5,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "DATA_DIR = BASE_DIR / \"knowledgebase\"\nDB_DIR = BASE_DIR / \"chroma_langchain_db_all\"\nembeddings = OllamaEmbeddings(model=\"llama3.2\")\npersist_dir = DB_DIR\n# add_documents = not os.path.exists(persist_dir) # If DB does not exist, create it.\nadd_documents = os.path.exists(persist_dir)\n# --- Texts Loaders ---\ndef load_docs_from_dir(dir_path: Path) -> List[Document]:\n    \"\"\"Load PDFs, DOCX, TXT/MD, and CSVs from a directory.\"\"\"\n    all_docs: List[Document] = []",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "DB_DIR",
        "kind": 5,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "DB_DIR = BASE_DIR / \"chroma_langchain_db_all\"\nembeddings = OllamaEmbeddings(model=\"llama3.2\")\npersist_dir = DB_DIR\n# add_documents = not os.path.exists(persist_dir) # If DB does not exist, create it.\nadd_documents = os.path.exists(persist_dir)\n# --- Texts Loaders ---\ndef load_docs_from_dir(dir_path: Path) -> List[Document]:\n    \"\"\"Load PDFs, DOCX, TXT/MD, and CSVs from a directory.\"\"\"\n    all_docs: List[Document] = []\n    for path in dir_path.rglob(\"*\"):",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "embeddings",
        "kind": 5,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "embeddings = OllamaEmbeddings(model=\"llama3.2\")\npersist_dir = DB_DIR\n# add_documents = not os.path.exists(persist_dir) # If DB does not exist, create it.\nadd_documents = os.path.exists(persist_dir)\n# --- Texts Loaders ---\ndef load_docs_from_dir(dir_path: Path) -> List[Document]:\n    \"\"\"Load PDFs, DOCX, TXT/MD, and CSVs from a directory.\"\"\"\n    all_docs: List[Document] = []\n    for path in dir_path.rglob(\"*\"):\n        if not path.is_file():",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "persist_dir",
        "kind": 5,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "persist_dir = DB_DIR\n# add_documents = not os.path.exists(persist_dir) # If DB does not exist, create it.\nadd_documents = os.path.exists(persist_dir)\n# --- Texts Loaders ---\ndef load_docs_from_dir(dir_path: Path) -> List[Document]:\n    \"\"\"Load PDFs, DOCX, TXT/MD, and CSVs from a directory.\"\"\"\n    all_docs: List[Document] = []\n    for path in dir_path.rglob(\"*\"):\n        if not path.is_file():\n            continue",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "add_documents",
        "kind": 5,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "add_documents = os.path.exists(persist_dir)\n# --- Texts Loaders ---\ndef load_docs_from_dir(dir_path: Path) -> List[Document]:\n    \"\"\"Load PDFs, DOCX, TXT/MD, and CSVs from a directory.\"\"\"\n    all_docs: List[Document] = []\n    for path in dir_path.rglob(\"*\"):\n        if not path.is_file():\n            continue\n        suffix = path.suffix.lower()\n        docs: List[Document] = []",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "raw_docs",
        "kind": 5,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "raw_docs = load_docs_from_dir(DATA_DIR)\nchunks = chunk(raw_docs)\n# --- Assign stable IDs ---\nids = []\nfor i, d in enumerate(chunks):\n    src = Path(d.metadata.get(\"source\", \"unknown\")).name\n    page = d.metadata.get(\"page\", 0)\n    start = d.metadata.get(\"start_index\", 0)\n    ids.append(f\"{src}#p{page}-s{start}-i{i}\")\n# --- Create or load Chroma DB ---",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "chunks",
        "kind": 5,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "chunks = chunk(raw_docs)\n# --- Assign stable IDs ---\nids = []\nfor i, d in enumerate(chunks):\n    src = Path(d.metadata.get(\"source\", \"unknown\")).name\n    page = d.metadata.get(\"page\", 0)\n    start = d.metadata.get(\"start_index\", 0)\n    ids.append(f\"{src}#p{page}-s{start}-i{i}\")\n# --- Create or load Chroma DB ---\nvector_store = Chroma(",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "ids",
        "kind": 5,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "ids = []\nfor i, d in enumerate(chunks):\n    src = Path(d.metadata.get(\"source\", \"unknown\")).name\n    page = d.metadata.get(\"page\", 0)\n    start = d.metadata.get(\"start_index\", 0)\n    ids.append(f\"{src}#p{page}-s{start}-i{i}\")\n# --- Create or load Chroma DB ---\nvector_store = Chroma(\n    collection_name=\"knowledge_mixed\",\n    persist_directory=persist_dir,",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "vector_store",
        "kind": 5,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "vector_store = Chroma(\n    collection_name=\"knowledge_mixed\",\n    persist_directory=persist_dir,\n    embedding_function=embeddings,\n)\n# # If DB already exists and you want to add new docs, set add_documents = True manually.\nif add_documents:\n    vector_store.add_documents(documents=raw_docs, ids=ids)\n# --- Build retriever ---\nretriever = vector_store.as_retriever(",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "retriever",
        "kind": 5,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "retriever = vector_store.as_retriever(\n    search_type=\"similarity\",  # or \"mmr\"\n    search_kwargs={\"k\": 5},\n)\n# --- Example query ---\nresults = retriever.invoke(\"not good pizza\")\nfor r in results:\n    print(r.metadata[\"source\"], r.page_content[:300], \"...\")",
        "detail": "vector_text",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "vector_text",
        "description": "vector_text",
        "peekOfCode": "results = retriever.invoke(\"not good pizza\")\nfor r in results:\n    print(r.metadata[\"source\"], r.page_content[:300], \"...\")",
        "detail": "vector_text",
        "documentation": {}
    }
]